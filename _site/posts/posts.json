[
  {
    "path": "posts/2021-07-04-new-chess-post/",
    "title": "Predicting chess games' outcome",
    "description": "On how I will never play the London System at 2am again.",
    "author": [
      {
        "name": "Robert Pikmets",
        "url": "https://github.com/robertpikmets"
      }
    ],
    "date": "2021-07-20",
    "categories": [],
    "contents": "\nIntroduction\nIt is 01:42 am, at a random night in 2018. I have just finished an online chess game, most likely while procrastinating on a school project. I played a blitz game as white, opting for the London system (1. e4 e5, 2. Bf4 …), which I did religiously play in that time period. The game lasted 34 moves and as usual, I was the one in time trouble. Regarding the awfully common occurrence of time management issues, I like to think I’m just too much of a perfectionist as opposed to just being slow - but who knows.\nThis random game, one of 4593 total games I have played on lichess.org over the years, caught my eye while pulling all of my chess data using the lichess API. Several questions came to mind:\nCan I predict the outcome with >50% accuracy?\nAm I actually slower than the average player?\nHow to maximise rating gain?\nWriting code\nPulling in data\nFirstly, I built this small piece of code that sends a request to the Lichess’ API, creates a .json file containing the games of some user and outputting this data to a data frame. In addition to the username, the function takes in a Personal Access Token (which can be created here: https://lichess.org/account/oauth/token) and a list of query specifications as function parameters. Query parameters are described here: https://lichess.org/api#operation/apiGamesUser. Pulling data works the fastest when the request is authenticated (with the token) and when downloading your own games - in that case, 60 games per second are downloaded. In my case, it takes about a minute. The requested JSON file is roughly 10 MB in size, so the following JSON streaming and data analysis should be completely manageable for my laptop that is soon of retirement age.\n\n\nlibrary(httr); library(jsonlite); library(tidyverse)\n\nget_chess_data <- function(username, token, query) {\n  url <- paste(\"https://lichess.org/api/games/user/\", username, sep = \"\")\n  \n  request <- GET(url, \n                 #default is PGN format, have to specify this in the ACCEPT \n                 #header for json data. an helper function from httr package is used\n                 accept(\"application/x-ndjson\"), \n                 #add token in the Authorization header\n                 add_headers(Authorization = paste(\"Bearer\", token, sep = \" \")),\n                 query = query)\n  \n  json_content <- content(request, as=\"text\", encoding = 'utf-8')\n  \n  #writes a file in current working directory\n  #write() function minifies the JSON records\n  write(json_content, \"chess_data.json\")\n  \n  #ndjson::stream_in is an alternative - that one requires only the path\n  #as input, jsonlite::stream_in requires file()\n  stream_data <- jsonlite::stream_in(file('chess_data.json'))\n  data <- jsonlite::flatten(stream_data)\n  return(data)\n}\n\n\n\nTo be more specific, the API actually outputs data in a NDJSON format, which stands for Newline-delimited JSON. NDJSON consists of individual lines where each line is any valid JSON text and each line is delimited with a newline character. While each individual line is valid JSON, the complete file as a whole is technically no longer valid JSON. Hence, it is a good idea to use jsonlite’s stream_in function for streaming the file in line-by-line. This is a pretty good approach actually as parsing a single huge JSON string would likely be more inefficient.\nArguments to the function are specified as such:\n\n\n#query parameters should be added as a list object\nquery <- list(rated = \"true\", \n              perfType = \"blitz,rapid,classical\",\n              clocks = \"true\",\n              pgnInJson = \"true\",\n              opening = \"true\")\n\nusername <- \"hotleafjuice\" #that's me, some good ol' Avatar: The Last Airbender reference\n\ntoken <- \"xxxxxxxxxxxxxx\" #hidden for security purposes\n\n\n\nI wanted to filter my games to only rated ones as this is likely to leave out any funny business which probably takes places while playing casual games with friends. I also left out bullet games - on my level, it’s the one with the fastest index finger who usually wins and it’s probably pointless to search for any real chess insight from these games. Furthermore, I wanted to get detailed clock information on each of the games to find out if I really am a super slow careful player. But as it turned out, this detailed clock information only comes along when I also set the pgnInJson parameter as true. Lastly, I want to see if my choice of opening can be a significant variable in predicting game outcome.\nSo let’s call the function and have a look at the output\n\n\ndata <- get_chess_data(username, token, query)\n\nstr(data %>% select(-moves, -pgn, -opening.name))\n\n\n\n\n'data.frame':   3304 obs. of  27 variables:\n $ id                       : chr  \"kit1D2Rq\" \"ztzoLozv\" \"NlJ0CwMY\" \"E5ik1mPk\" ...\n $ rated                    : logi  TRUE TRUE TRUE TRUE TRUE TRUE ...\n $ variant                  : chr  \"standard\" \"standard\" \"standard\" \"standard\" ...\n $ speed                    : chr  \"blitz\" \"blitz\" \"blitz\" \"blitz\" ...\n $ perf                     : chr  \"blitz\" \"blitz\" \"blitz\" \"blitz\" ...\n $ createdAt                : num  1.63e+12 1.63e+12 1.63e+12 1.63e+12 1.63e+12 ...\n $ lastMoveAt               : num  1.63e+12 1.63e+12 1.63e+12 1.63e+12 1.63e+12 ...\n $ status                   : chr  \"resign\" \"outoftime\" \"resign\" \"resign\" ...\n $ winner                   : chr  \"white\" \"white\" \"black\" \"white\" ...\n $ tournament               : chr  NA NA NA NA ...\n $ players.white.rating     : int  1834 1868 1848 1826 1803 1801 1835 1839 1832 1822 ...\n $ players.white.ratingDiff : int  4 9 -6 7 6 7 5 -4 7 6 ...\n $ players.white.provisional: logi  NA NA NA NA NA NA ...\n $ players.white.user.name  : chr  \"hotleafjuice\" \"ehlerdanlos\" \"Meeklydim\" \"hotleafjuice\" ...\n $ players.white.user.id    : chr  \"hotleafjuice\" \"ehlerdanlos\" \"meeklydim\" \"hotleafjuice\" ...\n $ players.white.user.patron: logi  NA NA NA NA NA NA ...\n $ players.black.rating     : int  1731 1840 1833 1832 1833 1840 1780 1936 1842 1839 ...\n $ players.black.ratingDiff : int  -5 -6 7 -7 -7 -7 -5 4 -7 -7 ...\n $ players.black.provisional: logi  NA NA NA NA NA NA ...\n $ players.black.user.name  : chr  \"cck80\" \"hotleafjuice\" \"hotleafjuice\" \"ahmad-ghanem\" ...\n $ players.black.user.id    : chr  \"cck80\" \"hotleafjuice\" \"hotleafjuice\" \"ahmad-ghanem\" ...\n $ players.black.user.patron: logi  NA NA NA NA NA NA ...\n $ opening.eco              : chr  \"B33\" \"B33\" \"B33\" \"B13\" ...\n $ opening.ply              : int  10 20 11 5 3 11 2 13 8 4 ...\n $ clock.initial            : int  300 180 180 180 180 300 300 300 300 300 ...\n $ clock.increment          : int  3 2 2 2 2 3 3 3 3 3 ...\n $ clock.totalTime          : int  420 260 260 260 260 420 420 420 420 420 ...\n\nAt the moment, I left out 3 variables from the structure call, simply because they contain really long strings and I could not figure out (in reasonable time) how to format the str() call such that the output would not exceed the blog website’s width all the way to the very right side.\nSo in total, I have 3304 observations (games) and 30 variables. Some of the variables are redundant, which I will leave out. Detailed clock information is inside the pgn variable - I will deal with this data later on.\nStay tuned folks, part 2 is coming soon..\n\n\n\n",
    "preview": {},
    "last_modified": "2021-07-22T02:30:28+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-20-electricity-prices/",
    "title": "TBATS for modelling electricity spot prices",
    "description": "How to handle multiple seasonality in data?",
    "author": [
      {
        "name": "Robert Pikmets",
        "url": "https://github.com/robertpikmets"
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\nIntroduction\nThis is another member of the “some of my past works” club. The task came once again from an external source and was originally undertaken in April 2020.\nProblem statement\nCalculate forecasted price for each hour of January 2020 (744 hours in total) assuming that average forecasted price for January 2020 is 40 EUR/MWh. Seasonality of electricity price must be taken into account when calculating hourly forecasted price.\nOriginal data contains hourly spot prices starting from beginning of 2006 until 17/04/2014 and can be used to model electricity price seasonality. Each day auction is held in order to determine hourly spot prices for the next day.\nApproach\nI’ll start by explaining some assumptions and simplifictions that have been made.\nFirstly, only data from 2010-2013 is used to train the model and the available data for 2014 as the test set. This is because most time series models do not work well for very long time series. Using data from 2006 is unlikely to provide any additional useful information about 2020’s prices, given there is no seasonality across decades. Furthermore, the model I’m using took way too much time and computing power to be created more than once for the whole of data, so there are good practical reasons for this as well.\nSecondly, the forecast for January 2020 is the same as for January 2015, but adjusted for average forecasted price of 40 EUR/MWh. This of course assumes stationarity, for which there is significant evidence according to Augmented Dickey-Fuller test. This approach is taken, because forecasting hourly data 6 years into the future is hardly viable or realistic, forecasting models are not well suited for that. Furthermore, this assumption of average price for January 2020 supports the idea of not forecasting years 2014-2019 and only then 2020.\nThirdly, January 2020 is assumed to be a rather usual month with no extreme values. One of the characteristic features of electricity markets are the abrupt and generally unanticipated extreme changes in the spot prices known as spikes. For example, original data contained 12 data points, for which the price exceeded 1000 EUR/MWh, which is rather unlikely given expected mean of 40 for January 2020. However, as Janczure et al. (2013) argue, extreme observations should be filtered and replaced in electricity spot prices before estimating any deterministic seasonal patters. Therefore, any outliers and also missing values have been dealt with by using STL (Seasonal and Trend decomposition using Loess decomposition). To estimate missing values and outlier replacements, linear interpolation is used on seasonally adjusted series.\nLastly, a key point in electricity spot price modeling and forecasting is the appropriate treatment of seasonality. The electricity price exhibits seasonality at three levels: daily, weekly and annual. I will try to model daily and weekly seasonality and assume that annual seasonality is captured by the 40 EUR/MWh assumption.\nMy approach is fitting the TBATS (Trigonometric Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components) model, introduced in De Livera, Hyndman, & Snyder (2011). I chose this model for two main reasons: it allows for modeling complex multiple seasonality and it works well for high frequency data, such as hourly.\nSteps shown below are:\nReading in and tidy data.\nCreating a time-series object with multiple seasonality - daily and weekly.\nTesting for stationarity.\nSplitting the data into training set and test set.\nFitting the TBATS model on training set.\nForecasting prices from beginning of 2014 to end of January 2015.\nForecast January 2020 prices.\nManipulate data to a tidy format and only including years 2010-2014.\n\n\nlibrary(readxl); library(tidyverse); library(forecast); library(xts)\nlibrary(lubridate); library(tseries)\ndata <- read_excel(\"hinnad.xlsx\")\n\n#pivot data from a wide to long format, we now have 3 columns only\ndata <- data %>% \n  rename(date = ...1) %>% \n  pivot_longer(-date, names_to = \"hour\", values_to = \"price\")\n\n#hourly date data from 2010 January to 2014 April\ntime_index <- seq(from = as.POSIXct(\"2010-01-01 00:00\", tz = \"UTC\"), \n                  to = as.POSIXct(\"2014-04-17 23:00\", tz = \"UTC\"), by = \"hour\")\nn <- length(time_index)\n\n#chooses only data from years 2010-2014, leaving earlier data out\ndata <- tail(data, n)\ndata$date <- time_index\n#data from 2010 onwards\nhead(data, n=3)\n\n\n# A tibble: 3 x 3\n  date                hour  price\n  <dttm>              <chr> <dbl>\n1 2010-01-01 00:00:00 1      39.6\n2 2010-01-01 01:00:00 2      39.5\n3 2010-01-01 02:00:00 3      39.1\n\nCreating a training and test set time-series objects with multiple seasonality - daily and weekly.\nNote that in the plots, it is always weeks on the x-axis.\n\n\n###Create the time series object\n#daily (24), weekly (24*7) seasonal periods\nmts <- msts(data$price, seasonal.periods = c(24, 24*7))\n#replaces outliers and missing values with previously explained method\nmts <- tsclean(mts)\n\n###Split the data\n#2014 data as test set\ntest_n <- length(seq(from = as.POSIXct(\"2014-01-01 00:00\", tz = \"UTC\"), \n                  to = as.POSIXct(\"2014-04-17 23:00\", tz = \"UTC\"), by = \"hour\"))\n#2010-2013 as training set\ntrain <- subset(mts, end=length(mts)-test_n)\ntest <- subset(mts, start=length(mts)-test_n+1)\n\n#lets see what our time-series looks like:\nplot(mts, xlab=\"Weeks\", ylab = \"Prices\")\n\n\n\n\nIs our time-series stationary, so that our forecasting assumptions hold?\n\n\nadf.test(mts) #data is stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  mts\nDickey-Fuller = -9.7998, Lag order = 33, p-value = 0.01\nalternative hypothesis: stationary\n\nWith a p-value of less than 0.01, we can reject the null hypothesis and accept the alternative hypothesis, which is that our time-series is stationary.\nLet’s also have a look at decomposed data to inspect the two types of seasonality.\n\n\n#decompose one month (January 2010) for visualisation purposes\nw <- window(train, end = time(train)[744])\n#just an example of how the seasonality looks like in January \n#seasonal24 is daily seasonality, seasonal168 is weekly seasonality\nautoplot(mstl(w))\n\n\n\n\nCreating the TBATS model:\n\n\n#create TBATS model\n#fit <- tbats(train) #<--- uncomment to actually fit the model\n#saveRDS(fit, file = \"fit.rds\") \n#Since it takes a lot of time, i'm loading a previously saved model\nfit <- readRDS(file = \"fit.rds\")\nfit\n\n\nTBATS(1, {0,0}, 0.8, {<24,5>, <168,6>})\n\nCall: tbats(y = train5)\n\nParameters\n  Alpha: 1.015974\n  Beta: -0.2241944\n  Damping Parameter: 0.800001\n  Gamma-1 Values: 0.01922611 0.001055555\n  Gamma-2 Values: 0.006996538 -0.001562361\n\nSeed States:\n            [,1]\n [1,] 46.3945353\n [2,] -2.5676674\n [3,] -5.3740340\n [4,] -1.2900871\n [5,]  0.8379294\n [6,]  0.5454669\n [7,] -0.3753072\n [8,] -1.8371109\n [9,] -2.9010861\n[10,]  0.1053905\n[11,]  0.1528641\n[12,] -0.0626940\n[13,]  0.4348419\n[14,]  1.9206361\n[15,] -0.7930991\n[16,]  0.9253493\n[17,] -0.9741675\n[18,] -0.5727657\n[19,] -3.5150505\n[20,]  1.1856272\n[21,]  0.3653065\n[22,]  0.1015183\n[23,]  1.1078930\n[24,] -1.5331099\n\nSigma: 3.235104\nAIC: 449338.9\n\nWhat does TBATS(1, {0,0}, 0.8, {<24,5>, <168,6>}) mean?\n1 - no Box-Cox transformation was used\n{0,0} - does not include ARMA errors\n0.8 - parameter of trend dampening\n<24,5> - daily seasonality is fitted with 5 Fourier terms\n<168,6> - weekly seasonality is fitted with 6 Fourier terms\nForecast prices from beginning of 2014 to end of January 2015.\n\n\n#number of data points\nh <- length(seq(from = as.POSIXct(\"2014-01-01 00:00\", tz = \"UTC\"), \n                  to = as.POSIXct(\"2015-01-31 23:00\", tz = \"UTC\"), by = \"hour\"))\nf <- forecast(fit, h=h)\n\n#Forecasted january 2014 - only small part of data is used here for better visualisation\nplot(forecast(fit, h=744), include = 744)\n\n\n\n#Plot two weeks of forecasted values from January 2014 against actual values\njan14_f <- window(f$mean, end = time(f$mean)[372])\njan14_test <- window(test, end = time(test)[372])\nautoplot(jan14_f, series=\"Forecasted values\") + autolayer(jan14_test, series=\"Actual values\") +\n  ylab(\"Prices\") + xlab(\"Weeks\")\n\n\n\n\nWe can see from the plot above that daily and weekly seasonality is captured some-what well, but the overall level of actual prices is much higher. This might be cause we are not estimating annual seasonality in our model or first 2 weeks of January 2014 just happened to be more volatile.\nWe can also calculate some error measures for the test set.\n\n\n###Accuracy of the model on the test set\nfit.test <- tbats(test, model=fit)\naccuracy(fit.test)\n\n\n                      ME     RMSE      MAE       MPE     MAPE\nTraining set -0.01085721 4.064628 2.716651 -1.121142 7.654116\n                  MASE        ACF1\nTraining set 0.9269896 0.005574984\n\nNow let’s come up with forecasts for January 2020. The forecast came out lower than 40 EUR/MWh, so shift all hourly prices higher equally.\n\n\n### Forecast for January 2020\n#find out the difference between assumed mean of 40 and forecasted average\ndiff <- 40-mean(tail(f$mean, n=744))\n#raise the price of each forecasted hourly price by that difference\npred <- tail(f$mean, n=744) + diff\n#confirm that the average of hourly prices is 40\nmean(pred)\n\n\n[1] 40\n\nplot(pred, main=\"Predicted prices for January 2020\", ylab=\"Prices\", xlab=\"Weeks\")\n\n\n\n\nLimitations\nI might be missing the idea of next day prices being determined at daily auctions. This could be implemented by only forecasting next 24 hours every day using available data, and then rolling the model forward by 24 hours, including the new observation. However, TBATS’ computational limitations (or perhaps more accurately, my computer’s computational limits) do not allow for this, so some other method, maybe a simpler ARIMA model could be considered. However such a model would probably then suffer from worse capability of modeling complex seasonality.\nI’m not sure how to include the assumption of average forecasted price for January 2020 being 40 EUR/MWh before actually creating the model, so the simplest approach of adding a constant to all hourly prices was chosen. However, I believe for forecasting hourly prices, it’s most important to model daily and weekly seasonality, which I think is done reasonably well even without including this average price assumption before-hand.\nOnly one model considered. Ideally we would consider many more models, implement cross-validation, compare model fit by AIC for example, compare test set errors, also model annual seasonality etc.\nNew comments\nImportant limitations were already outlined at the time of writing the piece, such as no comparison to other models (even to some simple benchmark model). The approach considered here only uses past information of the same variable for creating forecasts - in a way that I actually didn’t understand at the time (I hadn’t taken Time Series Analysis at school yet). And not even once did I look at the autocorrelations of the time series or error distribution of the model. It’s probably not a good idea to run some kind of “automatic model” and hope the model comes out reasonable. At the very least, I should know what all the parameters of the TBATS function are and set them in a way that makes sense. That said, as I was later doing research for my master’s thesis, I saw some papers on how the TBATS model generally performs pretty well in empirical comparisons of statistical models for forecasting electricity prices.\nAlso, I now have two new ideas for blog posts as well. One is about a time series project I did at school to demonstrate a more formal/structured approach to ARMA/ARIMA models. Second is a short overview of my master’s thesis, which is about forecasting intraday electricity prices on the Nord Pool. In my thesis, I used a very different methodology, based on using a lot of market information and developing LASSO models. Both posts can then compared to this earlier work and hopefully will demonstrate some developments in modelling skills.\nReferences\nDe Livera, Hyndman, & Snyder (2011): https://robjhyndman.com/papers/ComplexSeasonality.pdf\nJanczure et al. (2013): https://doi.org/10.1016/j.eneco.2013.03.013\n\n\n\n",
    "preview": "posts/2021-07-20-electricity-prices/electricity-prices_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-07-22T11:23:51+03:00",
    "input_file": "electricity-prices.knit.md"
  },
  {
    "path": "posts/2021-07-20-loan-applications/",
    "title": "Logistic regression vs random forest: loan applications",
    "description": "Only one can emerge victorious... right?",
    "author": [
      {
        "name": "Robert Pikmets",
        "url": "https://github.com/robertpikmets"
      }
    ],
    "date": "2021-07-12",
    "categories": [],
    "contents": "\nIntroduction\nBefore I’ll start on working on any new projects, I’ll post some of my older works. The following was done in February 2020 and it was actually a home task for a job application to a certain bank. Didn’t get the job, but I suppose it’s good enough for a first blog post. I’ll post it in its original form, but if I have any further comments now, going through it more than a year later, I’ll add them in the end.\nProblem statement\nThere is data on 2000 loan applications, of which 915 have been approved. Based on this data, 1000 new applications should be evaluated, out of which 500 should be accepted. Since the final outcome should be binary, either YES or NO, this is a classification problem. I will develop a logistic regression model and a random forest model, of which I shall choose the more suitable one to solve the problem.\nAdditionally, two questions should be answered:\nWhat’s the probability that client number 200 (Client_No=200) will fall under the category Got_Approval=YES?\nWhat’s the most important variable for approval?\nStep 1: Get data in, explore\n\n\n#first, load in necessary libraries\nlibrary(tidyverse); library(caret); library(GGally); library(pROC); library(lmtest)\nloan_data <- read.csv(\"decided_data.csv\", stringsAsFactors = TRUE) #decided upon applications\napplications <- read.csv(\"application_data.csv\", stringsAsFactors = TRUE) #new applications\n\nggpairs(loan_data %>% select(-Client_No))\n\n\n\n\nI used ggpairs() function to generate basic plots for all the column pairs in the data. This is a quick way to see if there are any dependencies and which variables seem to be important in explaining getting approval. Monthly income seems to be influencing it a lot, based on the box plot. Obligations looks relevant as well. However, it is unclear whether years worked is having an effect, so at this point, I’m a bit sceptical of this variable.\nStep 2: Splitting data\nThis is important for having independent data sets. Models will be trained only on 80% of the data, which can then be tested on 20% of remaining, out of sample data. Note that our loan data is actually already quite balanced, meaning YES and NO results are rather similar (915 vs 1085). CreateDataPartition() actually balances the splits on its own as well.\n\n\nset.seed(42)\nrowsToTrain <- createDataPartition(loan_data$Got_Approval, p=0.80, list=FALSE)\ntraining_set <- loan_data[rowsToTrain,]\ntest_set <- loan_data[-rowsToTrain,]\n\n\n\nStep 3: Best logistic regression model\nI created 3 logistic regression models on training set: model1 uses only monthly income to predict getting approval, then I added obligations to the second model and finally, included all three variables in model3.\n\n\nmodel1 <- glm(Got_Approval ~ Monthly_Income, data = training_set, family = binomial()) \nmodel2 <- glm(Got_Approval ~ Monthly_Income + Obligations, \n              data = training_set, family = binomial()) \nmodel3 <- glm(Got_Approval ~ Monthly_Income + Years_Worked + Obligations, \n              data = training_set, family = binomial()) \n\n\n\nFirst, I had a look at statistical significance of the independent variables in all models: they were all significant according to the P-value. Then, I compared Akaike information criterions (AIC) of the models, best fit represented by minimum AIC. AIC is good for making relative comparisions, it rewards goodness of fit and penalises complexity (i.e. overfitting). A rule of thumb, outlined in Burnham & Anderson (2004) is that if the difference between i-th model’s AIC and minimum AIC is higher than 10, i-th model has essentially no support.\n\n\nc(model1$aic, model2$aic, model3$aic)\n\n\n[1] 774.1353 610.0099 558.3376\n\nmodel2$aic - model3$aic > 10\n\n\n[1] TRUE\n\nAs an additional measure, I also conducted a likelihood ratio test (suitable for nested models), where a higher negative log likelihood (i.e. closer to 0) indicates a better fit.\n\n\nlrtest(model2, model3)$LogLik\n\n\n[1] -302.0049 -275.1688\n\nBoth tests agree with each other and provide evidence that model3 is the best logistic regression model. Whereas I was sceptical of including years worked in the model at first, evidence says otherwise.\nStep 4: Develop a random forest model; implement cross-validation\nRandom forest (RF) is a tree-based modelling approach, which combines many individual decision trees to vote for some class prediction. I will also implement k-fold cross-validation (CV), which is a tool for avoiding overfitting and validating the model on different subsets of the data. Note that for logistic regression (LR), CV does not actually change any parameters (as there aren’t any hyperparameters), so I’ve implemented it for evaluation purposes. I can check how the model might perform out of sample by looking at the variance and accuracies of each of the k fold. For random forest, CV also tunes its hyperparameter “mtry” to avoid overfitting.\n\n\ntrain_control <- trainControl(method=\"cv\", number=10)\n#same logistic regression model as model3, but cross-validated\nset.seed(42)\nlr_model <- train(Got_Approval ~ Monthly_Income + Years_Worked + Obligations,\n            data = training_set, trControl = train_control,method = \"glm\", family=binomial())\n#random forest model, same CV method. Includes all 3 variables of interest\nset.seed(42)\nrf_model <- train(Got_Approval ~ Monthly_Income + Years_Worked + Obligations,\n                  data = training_set, trControl = train_control, method =\"rf\", ntree=500)\n\n\n\nWhat we can see see is that the LR model actually achieves a little bit higher average accuracy of 92.2% across 10 folds, compared with 91.7% accuracy in case of RF. Note that this accuracy is not always the best metric to compare models, but in case of balanced data, one could say it’s not bad either.\n\n\nlr_model$results\n\n\n  parameter  Accuracy     Kappa AccuracySD    KappaSD\n1      none 0.9218539 0.8423123 0.02582722 0.05232445\n\nrf_model$results\n\n\n  mtry  Accuracy     Kappa AccuracySD    KappaSD\n1    2 0.9174791 0.8335099 0.02757180 0.05566334\n2    3 0.9118462 0.8222223 0.02452633 0.04947602\n\nStep 5: Make predictions, evaluate results\n\n\n#logistic regression predictions on test set using a 0.5 cutoff \nregr_prob <- predict(lr_model, newdata = test_set, type = \"prob\")\ntest_set$lr_prob <- regr_prob$YES #adds probabilities of YES as a column\ntest_set$lr_prediction <- ifelse(test_set$lr_prob > 0.5, \"YES\", \"NO\") #adds classification\ntest_set$lr_prediction <- as.factor(test_set$lr_prediction) \n\n\n\nHere it is very important to note that I have chosen a some-what arbitrary cut-off point of 0.5 for classification into either YES or NO. This takes a simplistic approach, saying that applications more probable to be approved, have been approved (i.e. >50%). Using some threshold makes strong assumptions about the underlying cost or utility functions, i.e. the consequences of decisions. In a credit context, giving out bad loans often has higher costs associated than not giving out good loans, meaning a higher threshold could be prefered. At this point, I don’t have enough info to set a better cut-off point than 0.5.\n\n\n#random forest predictions, which also use a 0.5 threshold\nforest_prob <- predict(rf_model, newdata = test_set, type = \"prob\")\ntest_set$rf_prob <- forest_prob$YES #adds probabilities of YES as a column\ntest_set$rf_prediction <- ifelse(test_set$rf_prob > 0.5, \"YES\", \"NO\") #adds classification\ntest_set$rf_prediction <- as.factor(test_set$rf_prediction) \n\n\n\nA good way to compare the models (in case of balanced data) is the ROC curve and Area Under Curve (AUC). A ROC curve summarizes the trade-off between the true positive rate and false positive rate using different probability thresholds, i.e. our previous cut-off point of 0.5 does not play a role. A higher AUC suggests the model is better at predicting.\n\n\nlr_roc = roc(test_set$Got_Approval ~ test_set$lr_prob, plot = FALSE)\nrf_roc = roc(test_set$Got_Approval ~ test_set$rf_prob, plot = FALSE)\nc(auc(lr_roc), auc(rf_roc))\n\n\n[1] 0.9790486 0.9813024\n\nplot(lr_roc)\n\n\n\nplot(rf_roc)\n\n\n\n\nWe get results very close to 1, RF model having only slightly higher AUC. Such high AUCs tell that both models are very good classifiers. Finally, as one of the questions is which variable is most important for approval, let’s check it:\n\n\n#logistic regression on the left, random forest on the right\ncbind(varImp(lr_model, scale=FALSE)[[1]],varImp(rf_model, scale=FALSE)[[1]])\n\n\n                 Overall   Overall\nMonthly_Income 15.646264 653.43736\nYears_Worked    6.873028  97.80676\nObligationsYES 10.848190  42.49536\n\nBoth models agree that monthly income is the most important variable. But interestingly, they disagree on the other two variables.\nSo what do make of all of this? In conclusion, both models have great predictive capabilities, the differences are rather minimal. I believe it is a matter of judgement, which model to use: random forest got slightly better results in the test set, however, logistic regression is simpler, computationally less taxing and more easily interpretable. For now, based on these advantages, I think it is reasonable to keep the logistic regression model to predict the loan approvals on new applications. The model chosen has such parameters:\n\n$coefficients\n   (Intercept) Monthly_Income   Years_Worked ObligationsYES \n  -28.44089703     0.02328615     0.19641250    -3.18654856 \n\nWhat this says is that monthly income and years worked have a positive effect on the log-odds of a loan being accepted, while having any previous obligations has a negative effect.\nStep 6: Classifying 1000 new applications\nSince I’ve been asked to strictly select 500 clients whom a loan should be given to, I will just choose the top 500 clients with highest probability of getting a loan. Please see the excel file for results.\n\n\nprobability <- predict(lr_model, newdata = applications, type = \"prob\")\napplications$probability <- probability$YES\napplications <- applications %>% arrange(desc(probability)) #reorder in terms of probability\napplications$decision[1:500] <- \"YES\"\napplications$decision[501:nrow(applications)] <- \"NO\"\napplications$decision <- as.factor(applications$decision)\n\n\n\nNote that the threshold in this case is roughly 0.47, meaning it may not actually be a good idea to approve a loan for all 500 clients (depends on the context). And finally, what’s the probability that client number 200 will get an approval? It should be roughly 2.9%.\n\n\nclient_200 <- applications %>% filter(Client_No == 200)\nclient_200$probability\n\n\n[1] 0.02863919\n\nComments\nOne thing I noticed is that I barely (actually not at all) talked about the method I used to evaluate variable importance. I’m pretty sure different metrics are being calculated for logistic regression and random forest and that should have been explained.\n\n\n\n",
    "preview": "posts/2021-07-20-loan-applications/loan-applications_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-07-22T10:45:05+03:00",
    "input_file": "loan-applications.knit.md"
  },
  {
    "path": "posts/2021-07-20-introduction/",
    "title": "Introduction",
    "description": "Hello world",
    "author": [
      {
        "name": "Robert Pikmets",
        "url": "https://github.com/robertpikmets"
      }
    ],
    "date": "2021-07-08",
    "categories": [],
    "contents": "\nHey what’s up, I’m Robert. In my free time, I like to hit the gym, play the piano, play chess and other board games and occasionally play some dota with my old classmates. More professionally speaking, I like to write code and wrangle, analyse, model and visualise data. It’s pretty cool to have millions of rows of seemingly random numbers or words, but with the help of statistics and machine learning, one can uncover its secrets and also peek into the future.\nThe reason for starting this blog is simple - to get better. At programming, machine learning and data visualisation. I don’t have a lot of formal computer science education, so I have some catching up to do. My bachelor’s was in Accounting & Finance, inspired from my endless fascination for the stock market when I was in high school. I still find it fascinating, but turns out I don’t have to know how to consolidate financial statements to gamble with invest in Gamestop. I pretty much knew after my first year at Warwick that I am way more passionate about data science. But by getting my master’s degree in Actuarial and Financial Engineering from the University of Tartu, combined with some online courses, I think I’ve made a successful transition. Studying at Tartu is definitely the most I’ve ever learned in two years. The curriculum wa heavy in calculus, probability theory and statistics and programming. That’s also where I first met R, which was love at first sight. I also had the chance to opt for electives that cover even more data science oriented topics, such as machine learning and databases.\nGoing forward, I will at first be working with R as that’s what I’m most comfortable with currently. Later on, I will also polish my Python skills with a deep learning project. “All models are wrong, but some are useful” as someone once said. So inevitably, with this blog, I will contribute to the former category, but hopefully also to the latter one.\n\nHow I really feel. Source: kaggle.com/general/123154\n\n\n\n",
    "preview": "posts/2021-07-20-introduction/images/print.jpeg",
    "last_modified": "2021-07-22T09:50:47+03:00",
    "input_file": {}
  }
]
